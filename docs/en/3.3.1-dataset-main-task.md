---
layout: page
permalink: /docs/en/dataset-main-task
key: dataset-en
sidebar:
  nav: dataset-en
---


<head>
    <style>
        .container {
            display: flex;
            justify-content: space-between; Creates space around items
        }

        .image-with-caption {
            width: 100%;
            margin: auto;
        }

        .image-with-caption img {
            width: 100%;
            height: auto;
        }

        .image-with-caption figcaption {
            text-align: center;
            font-size: 1em;
        }
    </style>
</head>



## Fingerspelling-Related Tasks

<figure class="image-with-caption">
    <img src="../assets/images/fs_tasks.png">
    <figcaption>Overview of fingerspelling-related tasks in our BANZ-FS dataset.</figcaption>
</figure>
<br>

We provide an overview of the \textbf{BANZSL-FS} benchmark tasks and their corresponding evaluation metrics:

<br>

### **Isolated Fingerspelling Recognition (IFSR)** [[ChicagoFSWild](#), [ChicagoFSWild+](#)]

Given a segmented fingerspelling clip
$\mathbb{V}_{fs} = \{I_{f_s}, ..., I_{f_e}\}$,
the goal of IFSR is to transcribe it into the corresponding letter sequence
$\hat{L} = \{l_1, ..., l_n\}$.

**Evaluation Metric**: **Letter Accuracy**
Defined as:

$$
1 - \frac{\text{EditDistance}(L^*, \hat{L})}{|L^*|}
$$

Where $L^*$ is the ground-truth letter sequence and $\hat{L}$ is the predicted sequence.
This edit-distance-based metric captures correctness, accounting for insertions, deletions, and substitutions.

---
<br>
### **Fingerspelling Detection (FSD)** [[ASLFS](#)]

Given an untrimmed sign language video
$\mathbb{V} = \{I_1, I_2, \ldots, I_T\}$
with $T$ frames, the goal of FSD is to identify temporal segments $(f_s, f_e)$
that localize fingerspelling intervals within $\mathbb{V}$.

**Evaluation Metric**: **AP@IoU**
- Average Precision is calculated based on temporal Intersection-over-Union (IoU) between predicted and ground-truth segments.
- Higher IoU thresholds reflect stricter localization accuracy.

---
<br>
### **Fingerspelling Detection followed by Recognition (FSD-R)** [[ASLFS](#)]

This is a two-stage approach:
1. An FSD model predicts temporal segments from an untrimmed sign language video
   $\mathbb{V} = \{I_1, I_2, ..., I_T\}$
2. Each predicted segment is processed by a fingerspelling recognizer to generate the corresponding letter sequence.

**Evaluation Metric**: **AP@Acc**
- Average Precision is computed using the **accuracy of the recognizer** on each predicted segment.
- A prediction is considered correct if its **recognition accuracy exceeds a defined threshold**, ensuring that detected segments are not only well-localized but also **interpretable**.

---

### **Fingerspelling Recognition in Context (FSR-Context)** [[Fleurs-asl-fs](#)]

Given a full sentence-level sign language video $\mathbb{V}$ and its predicted spoken language translation $\hat{T}$,
the task is to evaluate how accurately the model transcribes **fingerspelled terms embedded in the sentence**.

Fingerspelled spans annotated in the video are aligned with corresponding spans in $\hat{T}$, and **character-level accuracy** is measured.

**Evaluation Metric**: **Letter Accuracy**
<br>


### Baseline Models

<figure class="image-with-caption">
    <img src="../assets/images/fs_benchmarks.png">
    <figcaption>Overview of fingerspelling-related tasks in our BANZ-FS dataset.</figcaption>
</figure>
<br>

We mention that all models used in this work are publicly available. We express profound gratitude to the aforementioned authors for their invaluable contributions. Each of the ISLR models we use is linked below:

- **RGB-based & RGB-D-based model:**
  - ResNet2+1D [GitHub](https://github.com/leftthomas/R2Plus1D-C3D)
  - TSN [GitHub](https://github.com/yjxiong/temporal-segment-networks)
  - I3D [GitHub](https://github.com/google-deepmind/kinetics-i3d)
  - S3D [GitHub](https://github.com/kylemin/S3D)
  - SlowFast [GitHub](https://github.com/facebookresearch/SlowFast)
  - Timesformer [GitHub](https://github.com/facebookresearch/TimeSformer)
  - UMDR [GitHub](https://github.com/damo-cv/MotionRGBD)
  - KVNet-V [GitHub](https://github.com/FangyunWei/SLRT)

- **2D pose-based & 3D pose-based model:**
  - TGCN [GitHub](https://github.com/dxli94/WLASL)
  - SL-GCN [GitHub](https://github.com/jackyjsy/SAM-SLR-v2)
  - SPTOTER [GitHub](https://github.com/matyasbohacek/spoter)
  - KVNet-K [GitHub](https://github.com/FangyunWei/SLRT)

- **Multi-modal-based model:**
  - SAM-SLR [GitHub](https://github.com/jackyjsy/SAM-SLR-v2)
  - NLA-SLR [GitHub](https://github.com/FangyunWei/SLRT)

